{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Krista Shepard DS5 S7M1 Decision Trees.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KryssyCo/DS-Unit-2-Tree-Ensembles/blob/master/Krista_Shepard_DS5_S7M1_Decision_Trees.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QX0jgn6FTo2I"
      },
      "source": [
        "_Lambda School Data Science_\n",
        "\n",
        "This sprint, your project is about water pumps in Tanzania. Can you predict which water pumps are faulty?\n",
        "\n",
        "# Decision Trees\n",
        "\n",
        "#### Objectives\n",
        "- clean data with outliers\n",
        "- impute missing values\n",
        "- use scikit-learn for decision trees\n",
        "- understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions\n",
        "- get and interpret feature importances of a tree-based model\n",
        "\n",
        "#### Links\n",
        "\n",
        "- A Visual Introduction to Machine Learning\n",
        "  - [Part 1: A Decision Tree](http://www.r2d3.us/visual-intro-to-machine-learning-part-1/)\n",
        "  - [Part 2: Bias and Variance](http://www.r2d3.us/visual-intro-to-machine-learning-part-2/)\n",
        "- [Decision Trees: Advantages & Disadvantages](https://christophm.github.io/interpretable-ml-book/tree.html#advantages-2)\n",
        "- [How a Russian mathematician constructed a decision tree — by hand — to solve a medical problem](http://fastml.com/how-a-russian-mathematician-constructed-a-decision-tree-by-hand-to-solve-a-medical-problem/)\n",
        "- [How decision trees work](https://brohrer.github.io/how_decision_trees_work.html)\n",
        "- [Let’s Write a Decision Tree Classifier from Scratch](https://www.youtube.com/watch?v=LDRbO9a6XPU) — _Don’t worry about understanding the code, just get introduced to the concepts. This 10 minute video has excellent diagrams and explanations._\n",
        "- [Random Forests for Complete Beginners: The definitive guide to Random Forests and Decision Trees](https://victorzhou.com/blog/intro-to-random-forests/)\n",
        "\n",
        "### Libraries\n",
        "\n",
        "\n",
        "#### category_encoders\n",
        "\n",
        "You aren't required to use [category_encoders](https://github.com/scikit-learn-contrib/categorical-encoding), but it's recommended.\n",
        "\n",
        "If you're working locally, you already installed it, probably with this shell command: `conda install -c conda-forge category_encoders` \n",
        "\n",
        "If you're using Google Colab, you need to reinstall it every time you restart all runtimes: `pip install category_encoders`\n",
        "\n",
        "\n",
        "#### scikit-learn version 0.21.2\n",
        "\n",
        "Until recently, scikit-learn required graphviz to visualize decision trees, and it could be a pain to install. But sklearn's newest versions have a [plot_tree](https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html) function that uses matplotlib!\n",
        "\n",
        "Google Colab already has version 0.21.2. But if you're running Anaconda locally, you may need to upgrade.\n",
        "\n",
        "You can check your version with this Python code: `import sklearn; print(sklearn.__version__)`\n",
        "\n",
        "If necessary, you can update your version with this shell command: `conda update scikit-learn`\n",
        "\n",
        "This isn't required to do your assignment, but it's required to run this lecture notebook.\n",
        "\n",
        "#### pdpbox\n",
        "\n",
        "[PDPbox](https://github.com/SauceCat/PDPbox) stands for \"Partial Dependence Plot toolbox.\" It's a tool for model interpretation & visualization.\n",
        "\n",
        "You can install it on Colab or locally with this shell command: `pip install pdpbox`\n",
        "\n",
        "This also isn't required to do your assignment, but it's used in the lecture notebook."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hxCNx-MTySTI",
        "colab": {}
      },
      "source": [
        "!pip install pdpbox category_encoders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UFGmW4ijn4YN"
      },
      "source": [
        "## Clean data with outliers, impute missing values (example solutions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vt_SrDYHT4jC",
        "outputId": "e38c918f-1a3c-469c-bba9-3820a19fee83",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "!pip install category_encoders"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting category_encoders\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/a1/f7a22f144f33be78afeb06bfa78478e8284a64263a3c09b1ef54e673841e/category_encoders-2.0.0-py2.py3-none-any.whl (87kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 3.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.21.2)\n",
            "Requirement already satisfied: patsy>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.5.1)\n",
            "Requirement already satisfied: statsmodels>=0.6.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.10.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.16.4)\n",
            "Requirement already satisfied: pandas>=0.21.1 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.19.0 in /usr/local/lib/python3.6/dist-packages (from category_encoders) (1.3.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.20.0->category_encoders) (0.13.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from patsy>=0.4.1->category_encoders) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.5.0 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.21.1->category_encoders) (2.5.3)\n",
            "Installing collected packages: category-encoders\n",
            "Successfully installed category-encoders-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dg8hVHaJTldG",
        "outputId": "8ddd1e04-074d-456d-8951-926b44ae2b10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "%matplotlib inline\n",
        "import category_encoders as ce\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import RobustScaler\n",
        "\n",
        "LOCAL = '../data/tanzania/'\n",
        "WEB = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Linear-Models/master/data/tanzania/'\n",
        "\n",
        "train = pd.merge(pd.read_csv(WEB + 'train_features.csv'), \n",
        "                 pd.read_csv(WEB + 'train_labels.csv'))\n",
        "test = pd.read_csv(WEB + 'test_features.csv')\n",
        "sample_submission = pd.read_csv(WEB + 'sample_submission.csv')\n",
        "\n",
        "# Split train into train & val\n",
        "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
        "                              stratify=train['status_group'], random_state=42)\n",
        "\n",
        "train.shape, val.shape, test.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47520, 41), (11880, 41), (14358, 40))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DeIQ1lDmqAHY"
      },
      "source": [
        "Some of the locations are at [\"Null Island\"](https://en.wikipedia.org/wiki/Null_Island) instead of Tanzania."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qVltyo5bpac7",
        "outputId": "3b5e7599-ba00-4b2d-83d9-cf817f2c4b13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 441
        }
      },
      "source": [
        "sns.jointplot(x='longitude', y='latitude', data=train);"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa4AAAGoCAYAAAAerAGHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nheACAGQKwQUAyBSCCwCQKQQXACBT/j+9jB84uEiMBQAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x432 with 3 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V-OOtTMzqkhM"
      },
      "source": [
        "#### Define a function to wrangle train, validate, and test sets in the same way.\n",
        "\n",
        "Fix the location, and do more data cleaning and feature engineering."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "m81UBUsiaIzH",
        "colab": {}
      },
      "source": [
        "def wrangle(X):\n",
        "    \"\"\"Wrangles train, validate, and test sets in the same way\"\"\"\n",
        "    X = X.copy()\n",
        "    \n",
        "    # About 3% of the time, latitude has small values near zero,\n",
        "    # outside Tanzania, so we'll treat these values like zero.\n",
        "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
        "    \n",
        "    # When columns have zeros and shouldn't, they are like null values.\n",
        "    # So we will replace them with the column mean.\n",
        "    cols_with_zeros = ['construction_year', 'longitude', 'latitude']\n",
        "    for col in cols_with_zeros:\n",
        "        X[col] = X[col].replace(0, np.nan)\n",
        "        X[col] = X[col].fillna(X[col].mean())\n",
        "        \n",
        "    # Convert date_recorded to datetime\n",
        "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
        "    \n",
        "    # Extract year from date_recorded\n",
        "    X['year_recorded'] = X['date_recorded'].dt.year\n",
        "    \n",
        "    # quantity & quantity_group are duplicates, so drop one\n",
        "    X = X.drop(columns='quantity_group')\n",
        "    \n",
        "    # for categoricals with missing values, fill with the category 'MISSING'\n",
        "    categoricals = X.select_dtypes(exclude='number').columns\n",
        "    for col in categoricals:\n",
        "        X[col] = X[col].fillna('MISSING')\n",
        "    \n",
        "    return X\n",
        "\n",
        "\n",
        "train = wrangle(train)\n",
        "val = wrangle(val)\n",
        "test = wrangle(test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sMqlStuWqwde"
      },
      "source": [
        "Now the locations look better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JoIkLqHLpq3x",
        "colab": {}
      },
      "source": [
        "sns.relplot(x='longitude', y='latitude', hue='status_group', \n",
        "            data=train, alpha=0.1);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ypjt052Lrmgn"
      },
      "source": [
        "#### Select features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "666FUbXOXgL8",
        "colab": {}
      },
      "source": [
        "# The status_group column is the target\n",
        "target = 'status_group'\n",
        "\n",
        "# Get a dataframe with all train columns except the target & id\n",
        "train_features = train.drop(columns=[target, 'id'])\n",
        "\n",
        "# Get a list of the numeric features\n",
        "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
        "\n",
        "# Get a series with the cardinality of the nonnumeric features\n",
        "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
        "\n",
        "# Get a list of all categorical features with cardinality <= 50\n",
        "categorical_features = cardinality[cardinality <= 50].index.tolist()\n",
        "\n",
        "# Combine the lists \n",
        "features = numeric_features + categorical_features"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "moC9WikFrqJV"
      },
      "source": [
        "#### Encode categoricals, scale features, fit and score Logistic Regression model, make predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rUUq8Zt5X1bD",
        "colab": {}
      },
      "source": [
        "# Arrange data into X features matrix and y target vector \n",
        "X_train = train[features]\n",
        "y_train = train[target]\n",
        "X_val = val[features]\n",
        "y_val = val[target]\n",
        "X_test = test[features]\n",
        "\n",
        "# Encoder: fit_transform on train, transform on val & test\n",
        "encoder = ce.OneHotEncoder(use_cat_names=True)\n",
        "X_train_encoded = encoder.fit_transform(X_train)\n",
        "X_val_encoded = encoder.transform(X_val)\n",
        "X_test_encoded = encoder.transform(X_test)\n",
        "\n",
        "# Scaler: fit_transform on train, transform on val & test\n",
        "scaler = RobustScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
        "X_val_scaled = scaler.transform(X_val_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_encoded)\n",
        "\n",
        "# Model: Fit on train, score on val, predict on test\n",
        "model = LogisticRegression(solver='lbfgs', multi_class='auto', n_jobs=-1)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "print('Validation Accuracy', model.score(X_val_scaled, y_val))\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "# Write submission csv file\n",
        "submission = sample_submission.copy()\n",
        "submission['status_group'] = y_pred\n",
        "submission.to_csv('submission-02.csv', index=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7EQOYZtqr00F"
      },
      "source": [
        "#### Get and plot coefficients"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RFlfjFN1gyjI",
        "colab": {}
      },
      "source": [
        "coefficients = pd.Series(model.coef_[0], X_train_encoded.columns)\n",
        "plt.figure(figsize=(10,30))\n",
        "coefficients.sort_values().plot.barh(color='grey');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kMW9JbD6wZ28"
      },
      "source": [
        "## Use scikit-learn for decision trees"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0s7t37euwd41"
      },
      "source": [
        "### Compare a Logistic Regression with 2 features, longitude & latitude ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "44TUfQpeo64d",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yD31msBkwlVQ"
      },
      "source": [
        "### ... versus a Decision Tree Classifier with 2 features, longitude & latitude\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "X_xIa_1Kozfl",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KkV4TjcNAPzp"
      },
      "source": [
        "## Understand why decision trees are useful to model non-linear, non-monotonic relationships and feature interactions\n",
        "\n",
        "#### What does _(non)monotonic_ mean?!?!\n",
        "- See Figures 1-3 in Wikipedia's article, [Monotonic function](https://en.wikipedia.org/wiki/Monotonic_function)\n",
        "- See [World Population Growth, 1700-2010](https://ourworldindata.org/world-population-growth-past-future). World Population is non-linear and monotonic. Annual growth rate is non-linear and non-monotonic.\n",
        "- See [Accidents per Mile Driven, by Driver Age](http://howwedrive.com/2009/02/20/whats-the-real-risk-of-older-drivers/). This is non-linear and non-monotonic.\n",
        "\n",
        "#### What does _feature interactions_ mean?!?!\n",
        "- See the explanation in [_Interpretable Machine Learning_, Chapter 5.4.1, Feature Interaction](https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction).\n",
        "- See the exploration in this notebook, under the heading ***Interlude #2: Simple housing***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KxBwjWHq_0xK"
      },
      "source": [
        "### Visualize decision tree\n",
        "\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.plot_tree.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lXF2Jkl4_3Lv",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "N5GYg5Ze7de3"
      },
      "source": [
        "### Make 3 heatmaps, with longitude & latitude\n",
        "- Actual % of functional waterpumps\n",
        "- Decision Tree predicted probability of functional waterpumps\n",
        "- Logistic Regression predicted probability of functional waterpumps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l6uVMTX3u7fN",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5zlKegIZBbGr"
      },
      "source": [
        "### Interlude #1: predicting golf putts\n",
        "(1 feature, non-linear, regression)\n",
        "\n",
        "https://statmodeling.stat.columbia.edu/2008/12/04/the_golf_puttin/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GjvPHk-FBayo",
        "colab": {}
      },
      "source": [
        "columns = ['distance', 'tries', 'successes']\n",
        "data = [[2, 1443, 1346],\n",
        "        [3, 694, 577],\n",
        "        [4, 455, 337],\n",
        "        [5, 353, 208],\n",
        "        [6, 272, 149],\n",
        "        [7, 256, 136],\n",
        "        [8, 240, 111],\n",
        "        [9, 217, 69],\n",
        "        [10, 200, 67],\n",
        "        [11, 237, 75],\n",
        "        [12, 202, 52],\n",
        "        [13, 192, 46],\n",
        "        [14, 174, 54],\n",
        "        [15, 167, 28],\n",
        "        [16, 201, 27],\n",
        "        [17, 195, 31],\n",
        "        [18, 191, 33],\n",
        "        [19, 147, 20],\n",
        "        [20, 152, 24]]\n",
        "\n",
        "putts = pd.DataFrame(columns=columns, data=data)\n",
        "putts['rate of success'] = putts['successes'] / putts['tries']\n",
        "putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts');"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Tr-83jOEBsxK"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "x988kCamBpbn",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "putts_X = putts[['distance']]\n",
        "putts_y = putts['rate of success']\n",
        "lr = LinearRegression()\n",
        "lr.fit(putts_X, putts_y)\n",
        "print('R^2 Score', lr.score(putts_X, putts_y))\n",
        "ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "ax.plot(putts_X, lr.predict(putts_X));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MjLGrZjfFGD-"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EpFIetrsB2yc",
        "colab": {}
      },
      "source": [
        "import graphviz\n",
        "from ipywidgets import interact\n",
        "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
        "\n",
        "def viztree(decision_tree, feature_names):\n",
        "    dot_data = export_graphviz(decision_tree, out_file=None, feature_names=feature_names, \n",
        "                               filled=True, rounded=True)   \n",
        "    return graphviz.Source(dot_data)\n",
        "\n",
        "def putts_tree(max_depth=1):\n",
        "    tree = DecisionTreeRegressor(max_depth=max_depth)\n",
        "    tree.fit(putts_X, putts_y)\n",
        "    print('R^2 Score', tree.score(putts_X, putts_y))\n",
        "    ax = putts.plot('distance', 'rate of success', kind='scatter', title='Golf Putts')\n",
        "    ax.step(putts_X, tree.predict(putts_X), where='mid')\n",
        "    plt.show()\n",
        "    display(viztree(tree, feature_names=['distance']))\n",
        "\n",
        "interact(putts_tree, max_depth=(1,6,1));"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WBQJB0PVFtIE"
      },
      "source": [
        "### Interlude #2: Simple housing \n",
        "(2 features, regression)\n",
        "\n",
        "https://christophm.github.io/interpretable-ml-book/interaction.html#feature-interaction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SHQ6oh1uFzHO",
        "colab": {}
      },
      "source": [
        "columns = ['Price', 'Good Location', 'Big Size']\n",
        "\n",
        "data = [[300000, 1, 1], \n",
        "        [200000, 1, 0], \n",
        "        [250000, 0, 1], \n",
        "        [150000, 0, 0]]\n",
        "\n",
        "house = pd.DataFrame(columns=columns, data=data)\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cH8IVe6GGAsm"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Z9uMiKdzF6Ud",
        "colab": {}
      },
      "source": [
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "q7AASlWVGMD4"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sWlqO_usGKS8",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MZkaOREUGzYp"
      },
      "source": [
        "### Simple housing, with a twist: _Feature Interaction_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6HE0_pibGzJj",
        "colab": {}
      },
      "source": [
        "house.loc[0, 'Price'] = 400000\n",
        "house_X = house.drop(columns='Price')\n",
        "house_y = house['Price']\n",
        "house"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rh7ZJe7GHCSp"
      },
      "source": [
        "#### Compare Linear Regression ... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YYINxJkdG_Q2",
        "colab": {}
      },
      "source": [
        "lr = LinearRegression()\n",
        "lr.fit(house_X, house_y)\n",
        "print('R^2', lr.score(house_X, house_y))\n",
        "print('Intercept \\t', lr.intercept_)\n",
        "coefficients = pd.Series(lr.coef_, house_X.columns)\n",
        "print(coefficients.to_string())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9KIVsFO_HSmS"
      },
      "source": [
        "#### ... versus a Decision Tree Regressor"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r6JYZBZBHIX2",
        "colab": {}
      },
      "source": [
        "tree = DecisionTreeRegressor()\n",
        "tree.fit(house_X, house_y)\n",
        "print('R^2', tree.score(house_X, house_y))\n",
        "viztree(tree, feature_names=house_X.columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cUW3dabWAWPk"
      },
      "source": [
        "## Get and interpret feature importances of a tree-based model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-1_grdR-AVyJ",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oOgglh_kCt_c"
      },
      "source": [
        "# Assignment\n",
        "- Start a clean notebook, or continue with yesterday's assignment notebook.\n",
        "- Continue to participate in our Kaggle competition with the Tanzania Waterpumps data. \n",
        "- Do more exploratory data analysis, data cleaning, feature engineering, and feature selection.\n",
        "- Try a Decision Tree Classifier. \n",
        "- Submit new predictions.\n",
        "- Commit your notebook to your fork of the GitHub repo.\n",
        "\n",
        "\n",
        "## Stretch Goals\n",
        "- Create visualizations and share on Slack.\n",
        "- Read more about decision trees and tree ensembles. You can start with the links at the top of this notebook.\n",
        "- Try [scikit-learn pipelines](https://scikit-learn.org/stable/modules/compose.html):\n",
        "\n",
        "> Pipeline can be used to chain multiple estimators into one. This is useful as there is often a fixed sequence of steps in processing the data, for example feature selection, normalization and classification. Pipeline serves multiple purposes here:\n",
        "\n",
        "> - **Convenience and encapsulation.** You only have to call fit and predict once on your data to fit a whole sequence of estimators.\n",
        "> - **Joint parameter selection.** You can grid search over parameters of all estimators in the pipeline at once.\n",
        "> - **Safety.** Pipelines help avoid leaking statistics from your test data into the trained model in cross-validation, by ensuring that the same samples are used to train the transformers and predictors.\n"
      ]
    }
  ]
}